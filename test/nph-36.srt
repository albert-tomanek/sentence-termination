1
00:00:00,380 --> 00:00:04,540
So, should we do a video about the three laws of robotics, then?

2
00:00:04,620 --> 00:00:07,740
Because it keeps coming up in the comments.

3
00:00:07,740 --> 00:00:13,980
Okay, so the thing is, you won't hear serious AI researchers talking about the three laws of robotics

4
00:00:13,980 --> 00:00:16,080
because they don't work. They never worked.

5
00:00:16,300 --> 00:00:24,080
So I think people don't see the three laws talked about, because they're not serious.

6
00:00:24,080 --> 00:00:28,780
They haven't been relevant for a very long time and they're out of a science fiction book, you know?

7
00:00:30,720 --> 00:00:37,340
So, I'm going to do it. I want to be clear that I'm not taking these seriously, right?

8
00:00:38,180 --> 00:00:41,500
I'm going to talk about it anyway, because it needs to be talked about.

9
00:00:44,040 --> 00:00:50,200
So these are some rules that science fiction author Isaac Asimov came up with, in his stories,

10
00:00:51,120 --> 00:00:59,320
as an attempted sort of solution to the problem of making sure that artificial intelligence did

11
00:00:59,320 --> 00:01:00,980
what we want it to do.

12
00:01:00,980 --> 00:01:03,680
Shall we read them out then and see what they are?

13
00:01:03,680 --> 00:01:06,180
Oh yeah, I'll look them- Give me a second.

14
00:01:06,180 --> 00:01:08,440
I've looked them up. Okay, right, so they are:

15
00:01:08,440 --> 00:01:14,240
Law Number 1: A robot may not injure a human being or, through inaction allow a human being

16
00:01:14,240 --> 00:01:15,280
to come to harm.

17
00:01:15,280 --> 00:01:21,360
Law Number 2: A robot must obey orders given it by human beings except where such orders would

18
00:01:21,360 --> 00:01:23,100
conflict with the first law.

19
00:01:23,100 --> 00:01:29,240
Law Number 3: A robot must protect its own existence as long as such protection does not conflict

20
00:01:29,240 --> 00:01:31,240
with the first or second laws.

21
00:01:31,240 --> 00:01:33,180
I think there was a zeroth one later as well.

22
00:01:33,360 --> 00:01:40,620
Law 0: A robot may not harm humanity or, by inaction, allow humanity to come to harm.

23
00:01:40,780 --> 00:01:47,140
So it's weird that these keep coming up because, okay, so firstly they were made by someone

24
00:01:47,140 --> 00:01:52,100
who is writing stories, right? And they're optimized for story-writing.

25
00:01:52,520 --> 00:01:56,920
But they don't even work in the books, right? If you read the books, they're all about

26
00:01:56,920 --> 00:02:02,600
the ways that these rules go wrong, the various, various negative consequences.

27
00:02:02,860 --> 00:02:09,100
The most unrealistic thing, in my opinion, about the way Asimov did his stuff was

28
00:02:09,640 --> 00:02:13,760
the way that things go wrong and then get fixed, right?

29
00:02:13,760 --> 00:02:18,640
Most of the time, if you have a super-intelligence, that is doing something you don't want it to do,

30
00:02:19,200 --> 00:02:26,000
there's probably no hero who's going to save the day with cleverness. Real life doesn't work that way,

31
00:02:26,000 --> 00:02:32,480
generally speaking, right? Because they're written in English. How do you define these things?

32
00:02:32,680 --> 00:02:38,760
How do you define human without having to first take an ethical stand on almost every issue?

33
00:02:38,760 --> 00:02:42,540
And if human wasn't hard enough, you then have to define harm, right?

34
00:02:42,720 --> 00:02:48,480
And you've got the same problem again. Almost any definitions you give for those words,

35
00:02:48,480 --> 00:02:52,900
really solid, unambiguous definitions that don't rely on human intuition,

36
00:02:54,220 --> 00:03:01,220
result in weird quirks of philosophy, resulting in your AI doing something you really don't want it to do.

37
00:03:01,220 --> 00:03:06,700
The thing is, in order to encode that rule, "Don't allow a human being to come to harm",

38
00:03:06,700 --> 00:03:11,380
in a way that means anything close to what we intuitively understand it to mean,

39
00:03:11,500 --> 00:03:18,620
you would have to encode within the words 'human' and 'harm' the entire field of ethics, right?

40
00:03:18,620 --> 00:03:23,780
You have to solve ethics, comprehensively, and then use that to make your definitions.

41
00:03:23,780 --> 00:03:27,920
So it doesn't solve the problem, it pushes the problem back one step

42
00:03:27,920 --> 00:03:30,580
into now, well how do we define these terms?

43
00:03:30,580 --> 00:03:34,440
When I say the word human, you know what I mean, and that's not because either of us

44
00:03:34,440 --> 00:03:39,940
have a rigorous definition of what a human is. We've just sort of learned by general association

45
00:03:39,940 --> 00:03:43,520
what a human is, and then the word 'human' points to that structure in your brain,

46
00:03:43,520 --> 00:03:46,260
but I'm not really transferring the content to you.

47
00:03:46,680 --> 00:03:56,620
So, you can't just say 'human' in the utility function of an AI and have it know what that means.

48
00:03:56,620 --> 00:03:58,940
You have to specify. You have to come up with a definition.

49
00:03:58,940 --> 00:04:03,880
And it turns out that coming up with a definition, a good definition, of something like 'human'

50
00:04:04,000 --> 00:04:11,880
is extremely difficult, right? It's a really hard problem of, essentially, moral philosophy.

51
00:04:11,880 --> 00:04:16,360
You would think it would be semantics, but it really isn't because,

52
00:04:16,360 --> 00:04:20,180
okay, so we can agree that I'm a human and you're a human. That's fine.

53
00:04:20,180 --> 00:04:24,040
And that this, for example, is a table, and therefore not a human.

54
00:04:24,140 --> 00:04:29,260
You know, the easy stuff, the central examples of the classes are obvious.

55
00:04:29,260 --> 00:04:34,800
But, the edge cases, the boundaries of the classes, become really important.

56
00:04:34,800 --> 00:04:38,600
The areas in which we're not sure exactly what counts as a human.

57
00:04:38,600 --> 00:04:46,900
So, for example, people who haven't been born yet, in the abstract, like people who hypothetically

58
00:04:46,900 --> 00:04:48,940
could be born ten years in the future, do they count?

59
00:04:48,940 --> 00:04:54,220
People who are in a persistent vegetative state don't have any brain activity.

60
00:04:54,240 --> 00:05:02,240
Do they fully count as people? People who have died or unborn fetuses, right?

61
00:05:02,240 --> 00:05:06,940
I mean, there's a huge debate even going on as we speak about whether they count as people.

62
00:05:06,940 --> 00:05:11,480
The higher animals, you know, should we include maybe dolphins, chimpanzees, something like that?

63
00:05:11,480 --> 00:05:18,220
Do they have weight? And so it it turns out you can't program in, you can't make your specification

64
00:05:18,220 --> 00:05:21,900
of humans without taking an ethical stance on all of these issues.

65
00:05:21,900 --> 00:05:26,340
All kinds of weird, hypothetical edge cases become relevant when you're talking about

66
00:05:26,340 --> 00:05:31,080
a very powerful machine intelligence, which you otherwise wouldn't think of.

67
00:05:31,080 --> 00:05:34,520
So for example, let's say we say that dead people don't count as humans.

68
00:05:34,520 --> 00:05:39,260
Then you have an AI which will never attempt CPR. This person's died.

69
00:05:39,260 --> 00:05:41,320
They're gone, forget about it, done, right?

70
00:05:41,340 --> 00:05:47,560
Whereas we would say, no, hang on a second, they're only dead temporarily. We can bring them back, right?

71
00:05:47,560 --> 00:05:54,000
Okay, fine, so then we'll say that people who are dead, if they haven't been dead for- Well, how long?

72
00:05:54,000 --> 00:05:58,540
How long do you have to be dead for? I mean, if you get that wrong and you just say, oh it's fine,

73
00:05:58,540 --> 00:06:02,180
do try to bring people back once they're dead, then you may end up with a machine

74
00:06:02,180 --> 00:06:05,320
that's desperately trying to revive everyone who's ever died in all of history,

75
00:06:05,440 --> 00:06:09,280
because there are people who count who have moral weight.

76
00:06:09,280 --> 00:06:13,860
Do we want that? I don't know, maybe. But you've got to decide, right?

77
00:06:13,860 --> 00:06:18,480
And that's inherent in your definition of human. You have to take a stance on all kinds of moral issues

78
00:06:18,480 --> 00:06:23,780
that we don't actually know with confidence what the answer is, just to program the thing in.

79
00:06:23,920 --> 00:06:31,560
And then it gets even harder than that, because there are edge cases which don't exist right now.

80
00:06:31,560 --> 00:06:38,060
Like, talking about living people, dead people, unborn people, that kind of thing.

81
00:06:38,060 --> 00:06:42,600
Fine, animals. But there are all kinds of hypothetical things which could exist

82
00:06:42,620 --> 00:06:49,240
which may or may not count as human. For example, emulated or simulated brains, right?

83
00:06:49,240 --> 00:06:54,400
If you have a very accurate scan of someone's brain and you run that simulation, is that a person?

84
00:06:54,560 --> 00:06:55,680
Does that count?

85
00:06:57,100 --> 00:07:00,740
And whichever way you slice that, you get interesting outcomes.

86
00:07:01,220 --> 00:07:07,680
So, if that counts as a person, then your machine might be motivated to bring out a situation

87
00:07:07,680 --> 00:07:13,000
in which there are no physical humans because physical humans are very difficult to provide for.

88
00:07:13,000 --> 00:07:18,040
Whereas simulated humans, you can simulate their inputs and have a much nicer environment for everyone.

89
00:07:18,040 --> 00:07:22,600
Is that what we want? I don't know. Is it, maybe? I don't know.

90
00:07:24,160 --> 00:07:27,480
I don't think anybody does. But the point is, you're trying to write an AI here, right?

91
00:07:27,480 --> 00:07:30,720
You're an AI developer. You didn't sign up for this.

92
00:07:33,540 --> 00:07:37,120
We'd like to thank Audible.com for sponsoring this episode of Computerphile.

93
00:07:37,120 --> 00:07:41,240
And if you like books, check out Audible.com's huge range of audiobooks.

94
00:07:41,240 --> 00:07:46,060
And if you go to Audible.com/computerphile, there's a chance to download one for free.

95
00:07:46,060 --> 00:07:50,800
Callum Chase has written a book called Pandora's Brain, which is a thriller centered around

96
00:07:50,800 --> 00:07:56,080
artificial general intelligence, and if you like that story, then there's a supporting nonfiction book

97
00:07:56,080 --> 00:07:59,620
called Surviving AI which is also worth checking out.

98
00:07:59,620 --> 00:08:05,080
So thanks to Audible for sponsoring this episode of Computerphile. Remember, audible.com/computerphile.

99
00:08:05,080 --> 00:08:06,520
Download a book for free.

